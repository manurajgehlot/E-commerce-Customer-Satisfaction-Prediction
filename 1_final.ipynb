{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "QS5ArTQDHHoz",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "901a49c0-1e5f-4beb-a699-4d628cd8b755"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b51cb91c-f82f-4ca4-b313-e98ea4d6ae13\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b51cb91c-f82f-4ca4-b313-e98ea4d6ae13\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving combine.xlsx to combine.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "cf1=pd.read_csv(\"zxtrain.csv\",index_col=False)\n",
        "cf2=pd.read_csv(\"zxtest.csv\",index_col=False)\n",
        "cf2[\"review_comment_message\"].fillna(\"nao_reveja\",inplace=True)\n",
        "cf3=pd.read_csv(\"zytrain.csv\",index_col=False)\n",
        "cf4=pd.read_csv(\"/content/zytest.csv\",index_col=False)"
      ],
      "metadata": {
        "id": "KMT8_ArwfhKg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytest=list(cf4[\"class\"].values)"
      ],
      "metadata": {
        "id": "cXdcILdDGJsI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "metadata": {
        "id": "eNrvyhIlEyR0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "63ywcL3QuDzT"
      },
      "outputs": [],
      "source": [
        "def final_1(uploaded_file):\n",
        "    count=0\n",
        "    xls = pd.ExcelFile(uploaded_file)\n",
        "    geo= pd.read_excel(xls,'geo',index_col=\"Unnamed: 0\")\n",
        "    item=pd.read_excel(xls,'item',index_col=\"Unnamed: 0\")\n",
        "    customer= pd.read_excel(xls,'customer',index_col=\"Unnamed: 0\")\n",
        "    review=pd.read_excel(xls,'review',index_col=\"Unnamed: 0\")\n",
        "    orders= pd.read_excel(xls,'orders',index_col=\"Unnamed: 0\")\n",
        "    products=pd.read_excel(xls,'products',index_col=\"Unnamed: 0\")\n",
        "    sellers= pd.read_excel(xls,'sellers',index_col=\"Unnamed: 0\")\n",
        "    payment=pd.read_excel(xls,'payment',index_col=\"Unnamed: 0\")\n",
        "    model=pickle.load(open('zstack_model.pkl', 'rb'))\n",
        "\n",
        "    df=pd.merge(orders,payment,on=\"order_id\",how=\"inner\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df=pd.merge(df,customer,on=\"customer_id\",how=\"outer\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df=pd.merge(df,item,on=\"order_id\",how=\"inner\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df.drop_duplicates(subset=[\"order_id\"],inplace=True)\n",
        "    df=pd.merge(df,products,on=\"product_id\",how=\"inner\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df=pd.merge(df,review,on=\"order_id\",how=\"outer\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df=pd.merge(df,sellers,on=\"seller_id\",how=\"outer\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df.replace(to_replace=\"bebes\",\n",
        "              value=\"babies\")\n",
        "    df.replace(to_replace=\"ferramentas_jardim\",\n",
        "              value=\"garden_tools\")\n",
        "    df.replace(to_replace=\"informatica_acessorios\",\n",
        "              value=\"informatica_accessories\")\n",
        "    df[\"review_comment_message\"].fillna(\"nao_reveja\",inplace=True)\n",
        "    df.drop(columns=[\"review_answer_timestamp\",\"review_creation_date\",\"review_id\",\"review_comment_title\",\"order_delivered_carrier_date\"],axis=1,inplace=True)\n",
        "    df1=df.copy()\n",
        "    df1[\"review_score\"]=df1[\"review_score\"].apply(lambda x: 0 if x < 3 else 1)\n",
        "    df1[\"order_purchase_timestamp\"]=pd.to_datetime(df1[\"order_purchase_timestamp\"])\n",
        "    df1[\"order_approved_timestamp\"]=pd.to_datetime(df1[\"order_approved_at\"])\n",
        "    df1[\"order_purchase_date\"]=df1.order_purchase_timestamp.dt.date\n",
        "    df1[\"approval_date\"]=df1[\"order_approved_timestamp\"].dt.date\n",
        "    df1[\"order_estimated_delivery_date\"]=pd.to_datetime(df1.order_estimated_delivery_date).dt.date\n",
        "    df1[\"order_delivered_customer_date\"]=pd.to_datetime(df1.order_delivered_customer_date).dt.date\n",
        "    df1.drop_duplicates(subset=[\"order_id\",\"customer_id\",\"order_status\",\"order_status\",\"order_approved_at\",\"order_delivered_customer_date\",\"product_weight_g\",\"product_category_name_english\",\"review_comment_message\"],inplace=True)\n",
        "    print(\"data retained after doing doing data cleaning {:.1f}\".format((df1.shape[0]/df.shape[0])*100))\n",
        "    import nltk\n",
        "    nltk.download(\"stopwords\")\n",
        "    from nltk.corpus import stopwords\n",
        "    from tqdm import tqdm\n",
        "    from nltk.stem import RSLPStemmer\n",
        "    import re\n",
        "    nltk.download(\"rslp\")\n",
        "    stopwords_portugues=stopwords.words(\"portuguese\")\n",
        "    stopwords_portugues.remove(\"nem\")\n",
        "    stopwords_portugues.remove(\"não\")\n",
        "    stemmer=RSLPStemmer()\n",
        "    preprocessed_reviews = []\n",
        "    for sentance in tqdm(df1['review_comment_message'].values):\n",
        "        sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
        "        sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
        "        sentance=re.sub(\"[nN][ãaAÃ][oO][Ññ]|[nN]\",\"Negação\",sentance)\n",
        "        sentance=sentance.replace(\"\\\\r\",'')\n",
        "        sentance=sentance.replace(\"\\\\n\",'')\n",
        "        sentance = re.sub('[^A-Za-z]+',' ',sentance)\n",
        "        sentance = ' '.join(a for a in sentance.split() if a.lower not in stopwords_portugues)\n",
        "        sentance = ' '.join(stemmer.stem(e.lower())for e in sentance.split())\n",
        "        \n",
        "        preprocessed_reviews.append(sentance.strip())\n",
        "\n",
        "\n",
        "    f10=df1.groupby(\"order_id\").count()[\"product_id\"]\n",
        "    f10.df=pd.DataFrame()\n",
        "    f10.df[\"order_id\"]=f10.index\n",
        "    f10.df[\"F10_number_of_product_ordered\"]=f10.values\n",
        "    f9=df1.groupby(\"product_id\").count()[\"seller_id\"]\n",
        "    f9.df=pd.DataFrame()\n",
        "    f9.df[\"product_id\"]=f9.index\n",
        "    f9.df[\"number_of_sellers\"]=f9.values\n",
        "    df1[\"f16_purchase_month\"]=(df1[\"order_purchase_timestamp\"]).apply(lambda x: x.month)\n",
        "    df1[\"f14_delivery_days\"]=(df1[\"order_delivered_customer_date\"]-df1[\"order_purchase_date\"]).dt.days\n",
        "    df1=pd.merge(df1,f9.df,on=\"product_id\")\n",
        "    df1=pd.merge(df1,f10.df,on=\"order_id\")\n",
        "\n",
        "    df1[\"f1_delivery_estimated_days\"]=(df1[\"approval_date\"]-df1[\"order_estimated_delivery_date\"]).dt.days\n",
        "    df1[\"f2_delivery_actual_days\"]=(df1[\"approval_date\"]-df1[\"order_delivered_customer_date\"]).dt.days\n",
        "    df1[\"f3_on_time_or_late\"]=(df1[\"f1_delivery_estimated_days\"] > df1[\"f2_delivery_actual_days\"]).astype(\"int\")\n",
        "    df1[\"f4_discount_or_not\"]=(df1[\"payment_value\"]-(df1[\"price\"]+df1[\"freight_value\"])).apply(lambda x: 1 if x < 0 else 0)#converted to boolean \n",
        "    df1[\"f5_purchase_approved_delay\"]=(df1[\"approval_date\"]-df1[\"order_purchase_date\"]).dt.days\n",
        "    df1[\"f6_Average_product_value_per_order\"]=df1.price/(df1.F10_number_of_product_ordered)\n",
        "    df1[\"f7_order_frieght_value\"]=df1.freight_value/df1.price\n",
        "    df1[\"f8_purchase_day_week\"]=df1.order_purchase_timestamp.dt.dayofweek\n",
        "    df1[\"f11_order_purch_day\"]=df1.order_purchase_timestamp.dt.day\n",
        "    df1[\"f12_words_per_review\"]=df1.review_comment_message.apply(lambda x: len(x.split(\" \")))\n",
        "    df1[\"f13_order_purchase_hour\"]=df1[\"order_purchase_timestamp\"].apply(lambda x: x.hour)\n",
        "    df1[\"f15_week_delivery\"]=df1[\"order_purchase_timestamp\"].apply(lambda x: x.dayofweek)\n",
        "\n",
        "\n",
        "    df1[\"review_comment_message\"]=preprocessed_reviews\n",
        "\n",
        "\n",
        "    \n",
        "    xtest=df1.copy()\n",
        "    xtest.drop(['order_id'],axis=1,inplace=True)\n",
        "    xtest=xtest[[\"product_description_lenght\",\"product_name_lenght\",\"order_status\",\"payment_type\",\"product_category_name_english\",\"payment_installments\",\"product_weight_g\",\"price\",\"payment_sequential\",\"product_photos_qty\",\"f13_order_purchase_hour\",\"f6_Average_product_value_per_order\",\"payment_value\",\"f11_order_purch_day\",\"f12_words_per_review\",'f2_delivery_actual_days','f3_on_time_or_late', 'f4_discount_or_not','F10_number_of_product_ordered','review_score','review_comment_message']]\n",
        "    xtedon=xtest[['f3_on_time_or_late', 'f4_discount_or_not']]\n",
        "    \n",
        "    if 0<xtest['f6_Average_product_value_per_order'].values<7000:\n",
        "        norm=pickle.load(open('znorm1.pkl', 'rb'))\n",
        "        xte_apv=norm.transform(xtest['f6_Average_product_value_per_order'].values.reshape(-1,1))\n",
        "        count+=1\n",
        "    else:\n",
        "        print(\"enter correct value of average product value per order between 0-7000\")\n",
        "        xte_apv=0\n",
        "    \n",
        "    \n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    \n",
        "    if 3 < xtest[\"product_description_lenght\"].values < 4000:\n",
        "        norm=pickle.load(open('znorm3.pkl', 'rb'))\n",
        "        xte_pdl=norm.transform(xtest[\"product_description_lenght\"].values.reshape(-1,1))\n",
        "        count+=1\n",
        "    else:\n",
        "         print(\"enter correct product description length between 3-4000\")\n",
        "         xte_pdl=0\n",
        "        \n",
        "    if 4 < xtest[\"product_name_lenght\"].values < 80:\n",
        "        norm=pickle.load(open('znorm4.pkl', 'rb'))\n",
        "        xte_pnl=norm.transform(xtest[\"product_name_lenght\"].values.reshape(-1,1))\n",
        "        count+=1\n",
        "    \n",
        "    else:\n",
        "      print(\"enter correct product name length between 4-80\")\n",
        "      xte_pnl=0   \n",
        "   \n",
        "    if 0 < xtest[\"payment_value\"].values < 14000:\n",
        "        norm=pickle.load(open('znorm5.pkl', 'rb'))\n",
        "        xte_pv=norm.transform(xtest[\"payment_value\"].values.reshape(-1,1))\n",
        "        count+=1\n",
        "    else:\n",
        "        print(\"enter valid payment value between 0-14000\")\n",
        "        xte_pv=0\n",
        "    \n",
        "    if 0 < xtest[\"price\"].values < 7000:\n",
        "          norm=pickle.load(open('znorm7.pkl', 'rb'))\n",
        "          xte_pri=norm.transform(xtest['price'].values.reshape(-1,1))\n",
        "          count+=1\n",
        "    else:\n",
        "       print(\"enter the correct price between 0-7000\")\n",
        "       xte_pri=0\n",
        "    \n",
        "\n",
        "    if 0 < xtest['product_weight_g'].values < 41000:\n",
        "\n",
        "      norm=pickle.load(open('znorm8.pkl', 'rb'))\n",
        "      xte_pw=norm.transform(xtest['product_weight_g'].values.reshape(-1,1))\n",
        "      count+=1\n",
        "    \n",
        "    else:\n",
        "       print(\"enter correct product weight between 0-41000\")\n",
        "       xte_pw=0  \n",
        "    \n",
        "    if 0<=xtest['payment_installments'].values<=30:\n",
        "      norm=pickle.load(open('znorm9.pkl', 'rb'))\n",
        "      xte_pi=norm.transform(xtest['payment_installments'].values.reshape(-1,1))\n",
        "      count+=1\n",
        "    else:\n",
        "      print(\"correct payment insatllments 0-30\")\n",
        "      xte_pi=0\n",
        "\n",
        "    #here we are normalising the numerical features as so that all come to same scale\n",
        "    norm=pickle.load(open('znorm10.pkl', 'rb'))\n",
        "    xte_dad=norm.transform(xtest[\"f2_delivery_actual_days\"].values.reshape(-1,1))\n",
        "     \n",
        "\n",
        "    norm=pickle.load(open('norm11.pkl', 'rb'))\n",
        "    xte_opd=norm.transform(xtest[\"f11_order_purch_day\"].values.reshape(-1,1))\n",
        "    \n",
        "    if 0< xtest[\"f12_words_per_review\"].values <60:\n",
        "      norm=pickle.load(open('znorm12.pkl', 'rb'))\n",
        "      xte_wpr=norm.transform(xtest[\"f12_words_per_review\"].values.reshape(-1,1))\n",
        "      count+=1\n",
        "    else:\n",
        "      print(\"enter limited words in review comment message between  0-60 \")\n",
        "      xte_wpr=0\n",
        "    \n",
        "    \n",
        "    if 0< xtest['F10_number_of_product_ordered'].values < 10:\n",
        "\n",
        "      norm=pickle.load(open('znorm13.pkl', 'rb'))\n",
        "      xte_po=norm.transform(xtest['F10_number_of_product_ordered'].values.reshape(-1,1))\n",
        "      count+=1\n",
        "    else:\n",
        "      print(\"enter correct number of products ordered between 0-10 \")\n",
        "      xte_po=0\n",
        "\n",
        "\n",
        "    vect=pickle.load(open('zvect.pkl', 'rb'))\n",
        "    xte_os=vect.transform(xtest[\"order_status\"].values)\n",
        "\n",
        "\n",
        "\n",
        "    vect=pickle.load(open('zvect1.pkl', 'rb'))\n",
        "    xte_pty=vect.transform(xtest[\"payment_type\"].values)\n",
        "\n",
        "\n",
        "    vect=pickle.load(open('zvect2.pkl', 'rb'))\n",
        "    xte_pcm=vect.transform(xtest[\"product_category_name_english\"].values)\n",
        "\n",
        "    norm=pickle.load(open('znorm2.pkl', 'rb'))\n",
        "    xte_psq=norm.transform(xtest['payment_sequential'].values.reshape(-1,1))\n",
        "    \n",
        "    norm=pickle.load(open('znorm6.pkl', 'rb'))\n",
        "    xte_phr=norm.transform(xtest['f13_order_purchase_hour'].values.reshape(-1,1))\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "    #here we are vectorising the text data using tfidf such that we are ignoring the terms which are appearing less than 3 times\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    vectorizer =pickle.load(open('zvectorizer.pkl', 'rb')) \n",
        "    xte_rcm=vectorizer.transform(xtest[\"review_comment_message\"].values)\n",
        "    \n",
        "    #we are stacking the horizontally the all the vectors formed\n",
        "    from scipy.sparse import hstack\n",
        "    xte=hstack((xte_pcm,xte_pty,xte_os,xte_pnl,xte_pdl,xte_apv,xte_psq,xte_pv,xte_phr,xte_pri,xte_pw,xte_pi,xte_dad,xte_wpr,xte_opd,xtedon,xte_po,xte_rcm)).tocsr()\n",
        "    \n",
        "    if count==9:\n",
        "      val=model.predict(xte)\n",
        "    if val==1:\n",
        "        return \"positive review given\"\n",
        "    if val==0:\n",
        "        return \"negative review given\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred=final_1('pos_1.xlsx')\n",
        "print(\"*\"*10)\n",
        "print(\"\\noutput is\",pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r954NvOBE1C4",
        "outputId": "e7ccfdd5-c1f1-4f28-8b92-05b1dc90d0dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data retained after doing doing data cleaning 100.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n",
            "100%|██████████| 1/1 [00:00<00:00, 1101.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********\n",
            "\n",
            "output is positive review given\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def final_2(uploaded_file):\n",
        "    count=0\n",
        "    xls = pd.ExcelFile(uploaded_file)\n",
        "    geo= pd.read_excel(xls,'geo',index_col=\"Unnamed: 0\")\n",
        "    item=pd.read_excel(xls,'item',index_col=\"Unnamed: 0\")\n",
        "    customer= pd.read_excel(xls,'customer',index_col=\"Unnamed: 0\")\n",
        "    review=pd.read_excel(xls,'review',index_col=\"Unnamed: 0\")\n",
        "    orders= pd.read_excel(xls,'orders',index_col=\"Unnamed: 0\")\n",
        "    products=pd.read_excel(xls,'products',index_col=\"Unnamed: 0\")\n",
        "    sellers= pd.read_excel(xls,'sellers',index_col=\"Unnamed: 0\")\n",
        "    payment=pd.read_excel(xls,'payment',index_col=\"Unnamed: 0\")\n",
        "    name_translation=pd.read_excel(xls,'name_translation',index_col=\"Unnamed: 0\")\n",
        "    \n",
        "    df=pd.merge(orders,payment,on=\"order_id\",how=\"outer\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df=pd.merge(df,customer,on=\"customer_id\",how=\"outer\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df=pd.merge(df,item,on=\"order_id\",how=\"outer\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df.drop_duplicates(subset=[\"order_id\"],inplace=True)\n",
        "    df=pd.merge(df,products,on=\"product_id\",how=\"outer\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df=pd.merge(df,review,on=\"order_id\",how=\"outer\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df=pd.merge(df,sellers,on=\"seller_id\",how=\"inner\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df=pd.merge(df,name_translation,on=\"product_category_name\",how=\"outer\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    df1=df.copy()\n",
        "    df1[\"order_approved_at\"].fillna(value=df1[\"order_purchase_timestamp\"],inplace=True)\n",
        "    df1[\"order_delivered_customer_date\"].fillna(value=df1[\"order_estimated_delivery_date\"],inplace=True)\n",
        "    df1[\"price\"].fillna(value=df1[\"price\"].median(),inplace=True)\n",
        "    df1[\"freight_value\"].fillna(value=df1[\"freight_value\"].median(),inplace=True)\n",
        "    df1[\"product_name_lenght\"].fillna(value=df1[\"product_name_lenght\"].median(),inplace=True)\n",
        "    df1[\"product_description_lenght\"].fillna(value=df1[\"product_description_lenght\"].median(),inplace=True)\n",
        "    df1[\"product_photos_qty\"].fillna(value=df1[\"product_photos_qty\"].median(),inplace=True)\n",
        "    df1[\"product_weight_g\"].fillna(value=df1[\"product_weight_g\"].median(),inplace=True)\n",
        "    df1[\"product_height_cm\"].fillna(value=df1[\"product_height_cm\"].median(),inplace=True)\n",
        "    df1[\"product_length_cm\"].fillna(value=df1[\"product_length_cm\"].median(),inplace=True)\n",
        "    df1[\"product_width_cm\"].fillna(value=df1[\"product_width_cm\"].median(),inplace=True)\n",
        "    df1[\"payment_value\"].fillna(value=df1[\"payment_value\"].median(),inplace=True)\n",
        "    df1[\"review_score\"].fillna(value=df1[\"review_score\"].median(),inplace=True)\n",
        "    df1[\"review_comment_message\"].fillna(\"nao_reveja\",inplace=True)\n",
        "    df1[\"payment_sequential\"].fillna(value=df1[\"payment_sequential\"].value_counts().index[0],inplace=True)\n",
        "    df1[\"payment_type\"].fillna(value=df1[\"payment_type\"].value_counts().index[0],inplace=True)\n",
        "\n",
        "    df1[\"payment_installments\"].fillna(value=df1[\"payment_installments\"].value_counts().index[0],inplace=True)\n",
        "    df1[\"product_category_name\"].fillna(value=df1[\"product_category_name\"].value_counts().index[21],inplace=True)\n",
        "    df1[\"product_category_name_english\"].fillna(value=df1[\"product_category_name_english\"].value_counts().index[21],inplace= True)\n",
        "    df1[\"order_item_id\"].fillna(value=df1[\"order_item_id\"].value_counts().index[0],inplace=True)\n",
        "    df1[\"seller_zip_code_prefix\"].fillna(value=df1[\"seller_zip_code_prefix\"].value_counts().index[0],inplace=True)\n",
        "    df1[\"seller_city\"].fillna(value=df1[\"seller_city\"].value_counts().index[0],inplace=True)\n",
        "    df1[\"seller_state\"].fillna(value=df1[\"seller_state\"].value_counts().index[0],inplace=True)\n",
        "    df1.drop(columns=[\"review_answer_timestamp\",\"review_creation_date\",\"review_id\",\"review_comment_title\",\"order_delivered_carrier_date\",\"product_category_name\"],axis=1,inplace=True)\n",
        "    df1.dropna(inplace=True)\n",
        "    df1.drop_duplicates(subset=[\"order_id\",\"customer_id\",\"order_status\",\"order_status\",\"order_approved_at\",\"order_delivered_customer_date\",\"product_weight_g\",\"product_category_name_english\",\"review_comment_message\"],inplace=True)\n",
        "    print(\"data retained after doing doing data cleaning {:.1f}\".format((df1.shape[0]/df.shape[0])*100))\n",
        "    df1[\"review_score\"]=df1[\"review_score\"].apply(lambda x: 0 if x < 3 else 1)\n",
        "    df1[\"order_purchase_timestamp\"]=pd.to_datetime(df1[\"order_purchase_timestamp\"])\n",
        "    df1[\"order_approved_timestamp\"]=pd.to_datetime(df1[\"order_approved_at\"])\n",
        "    df1[\"order_purchase_date\"]=df1.order_purchase_timestamp.dt.date\n",
        "    df1[\"approval_date\"]=df1[\"order_approved_timestamp\"].dt.date\n",
        "    df1[\"order_estimated_delivery_date\"]=pd.to_datetime(df1.order_estimated_delivery_date).dt.date\n",
        "    df1[\"order_delivered_customer_date\"]=pd.to_datetime(df1.order_delivered_customer_date).dt.date \n",
        "\n",
        "\n",
        "\n",
        "    import nltk\n",
        "    nltk.download(\"stopwords\")\n",
        "    from nltk.corpus import stopwords\n",
        "    from tqdm import tqdm\n",
        "    from nltk.stem import RSLPStemmer\n",
        "    import re\n",
        "    nltk.download(\"rslp\")\n",
        "    stopwords_portugues=stopwords.words(\"portuguese\")\n",
        "    stopwords_portugues.remove(\"nem\")\n",
        "    stopwords_portugues.remove(\"não\")\n",
        "    stemmer=RSLPStemmer()\n",
        "    preprocessed_reviews = []\n",
        "    for sentance in tqdm(df1['review_comment_message'].values):\n",
        "        sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
        "        sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
        "        sentance=re.sub(\"[nN][ãaAÃ][oO][Ññ]|[nN]\",\"Negação\",sentance)\n",
        "        sentance=sentance.replace(\"\\\\r\",'')\n",
        "        sentance=sentance.replace(\"\\\\n\",'')\n",
        "        sentance = re.sub('[^A-Za-z]+',' ',sentance)\n",
        "        sentance = ' '.join(a for a in sentance.split() if a.lower not in stopwords_portugues)\n",
        "        sentance = ' '.join(stemmer.stem(e.lower())for e in sentance.split())\n",
        "        \n",
        "        preprocessed_reviews.append(sentance.strip())\n",
        "\n",
        "\n",
        "    df1[\"review_comment_message\"]=preprocessed_reviews\n",
        "    f10=df1.groupby(\"order_id\").count()[\"product_id\"]\n",
        "    f10.df=pd.DataFrame()\n",
        "    f10.df[\"order_id\"]=f10.index\n",
        "    f10.df[\"F10_number_of_product_ordered\"]=f10.values\n",
        "    f9=df1.groupby(\"product_id\").count()[\"seller_id\"]\n",
        "    f9.df=pd.DataFrame()\n",
        "    f9.df[\"product_id\"]=f9.index\n",
        "    f9.df[\"number_of_sellers\"]=f9.values\n",
        "    df1[\"f16_purchase_month\"]=(df1[\"order_purchase_timestamp\"]).apply(lambda x: x.month)\n",
        "    df1[\"f14_delivery_days\"]=(df1[\"order_delivered_customer_date\"]-df1[\"order_purchase_date\"]).dt.days\n",
        "    df1=pd.merge(df1,f9.df,on=\"product_id\")\n",
        "    df1=pd.merge(df1,f10.df,on=\"order_id\")\n",
        "    \n",
        "    df1[\"f1_delivery_estimated_days\"]=(df1[\"approval_date\"]-df1[\"order_estimated_delivery_date\"]).dt.days\n",
        "    df1[\"f2_delivery_actual_days\"]=(df1[\"approval_date\"]-df1[\"order_delivered_customer_date\"]).dt.days\n",
        "    df1[\"f3_on_time_or_late\"]=(df1[\"f1_delivery_estimated_days\"] > df1[\"f2_delivery_actual_days\"]).astype(\"int\")\n",
        "    df1[\"f4_discount_or_not\"]=(df1[\"payment_value\"]-(df1[\"price\"]+df1[\"freight_value\"])).apply(lambda x: 1 if x < 0 else 0)#converted to boolean \n",
        "    df1[\"f5_purchase_approved_delay\"]=(df1[\"approval_date\"]-df1[\"order_purchase_date\"]).dt.days\n",
        "    df1[\"f6_Average_product_value_per_order\"]=df1.price/(df1.F10_number_of_product_ordered)\n",
        "    df1[\"f7_order_frieght_value\"]=df1.freight_value/df1.price\n",
        "    df1[\"f8_purchase_day_week\"]=df1.order_purchase_timestamp.dt.dayofweek\n",
        "    df1[\"f11_order_purch_day\"]=df1.order_purchase_timestamp.dt.day\n",
        "    df1[\"f12_words_per_review\"]=df1.review_comment_message.apply(lambda x: len(x.split(\" \")))\n",
        "    df1[\"f13_order_purchase_hour\"]=df1[\"order_purchase_timestamp\"].apply(lambda x: x.hour)\n",
        "    df1[\"f15_week_delivery\"]=df1[\"order_purchase_timestamp\"].apply(lambda x: x.dayofweek)\n",
        "    df2=df1.copy()\n",
        "    df2=df2[[\"order_id\",\"product_description_lenght\",\"product_name_lenght\",\"order_status\",\"payment_type\",\"product_category_name_english\",\"payment_installments\",\"product_weight_g\",\"price\",\"payment_sequential\",\"product_photos_qty\",\"f13_order_purchase_hour\",\"f6_Average_product_value_per_order\",\"payment_value\",\"f11_order_purch_day\",\"f12_words_per_review\",'f2_delivery_actual_days','f3_on_time_or_late', 'f4_discount_or_not','F10_number_of_product_ordered','review_score','review_comment_message']]\n",
        "    df2 = df2.loc[:,~df2.columns.duplicated()].copy()\n",
        "    y=df2[\"review_score\"].values\n",
        "    x=df2.drop(\"review_score\",axis=1)\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    xtrain,xtest,ytrain,ytest=cf1,cf2,cf3,list(cf4[\"class\"].values)\n",
        "    xtedon=xtest[['f3_on_time_or_late', 'f4_discount_or_not']]\n",
        "    \n",
        "    \n",
        "    norm=pickle.load(open('znorm1.pkl', 'rb'))\n",
        "    xte_apv=norm.transform(xtest['f6_Average_product_value_per_order'].values.reshape(-1,1))\n",
        "    count+=1\n",
        "    \n",
        "    \n",
        "    norm=pickle.load(open('znorm3.pkl', 'rb'))\n",
        "    xte_pdl=norm.transform(xtest[\"product_description_lenght\"].values.reshape(-1,1))\n",
        "    count+=1\n",
        "        \n",
        "    norm=pickle.load(open('znorm4.pkl', 'rb'))\n",
        "    xte_pnl=norm.transform(xtest[\"product_name_lenght\"].values.reshape(-1,1))\n",
        "    count+=1\n",
        "    \n",
        "    norm=pickle.load(open('znorm5.pkl', 'rb'))\n",
        "    xte_pv=norm.transform(xtest[\"payment_value\"].values.reshape(-1,1))\n",
        "    count+=1\n",
        "    \n",
        "    norm=pickle.load(open('znorm7.pkl', 'rb'))\n",
        "    xte_pri=norm.transform(xtest['price'].values.reshape(-1,1))\n",
        "    count+=1\n",
        "  \n",
        "    norm=pickle.load(open('znorm8.pkl', 'rb'))\n",
        "    xte_pw=norm.transform(xtest['product_weight_g'].values.reshape(-1,1))\n",
        "    count+=1\n",
        "    \n",
        "    norm=pickle.load(open('znorm9.pkl', 'rb'))\n",
        "    xte_pi=norm.transform(xtest['payment_installments'].values.reshape(-1,1))\n",
        "    count+=1\n",
        "   \n",
        "    #here we are normalising the numerical features as so that all come to same scale\n",
        "    norm=pickle.load(open('znorm10.pkl', 'rb'))\n",
        "    xte_dad=norm.transform(xtest[\"f2_delivery_actual_days\"].values.reshape(-1,1))\n",
        "     \n",
        "\n",
        "    norm=pickle.load(open('norm11.pkl', 'rb'))\n",
        "    xte_opd=norm.transform(xtest[\"f11_order_purch_day\"].values.reshape(-1,1))\n",
        "    \n",
        "    norm=pickle.load(open('znorm12.pkl', 'rb'))\n",
        "    xte_wpr=norm.transform(xtest[\"f12_words_per_review\"].values.reshape(-1,1))\n",
        "    count+=1\n",
        "    norm=pickle.load(open('znorm13.pkl', 'rb'))\n",
        "    xte_po=norm.transform(xtest['F10_number_of_product_ordered'].values.reshape(-1,1))\n",
        "    count+=1\n",
        "  \n",
        "    vect=pickle.load(open('zvect.pkl', 'rb'))\n",
        "    xte_os=vect.transform(xtest[\"order_status\"].values)\n",
        "\n",
        "\n",
        "\n",
        "    vect=pickle.load(open('zvect1.pkl', 'rb'))\n",
        "    xte_pty=vect.transform(xtest[\"payment_type\"].values)\n",
        "\n",
        "\n",
        "    vect=pickle.load(open('zvect2.pkl', 'rb'))\n",
        "    xte_pcm=vect.transform(xtest[\"product_category_name_english\"].values)\n",
        "\n",
        "    norm=pickle.load(open('znorm2.pkl', 'rb'))\n",
        "    xte_psq=norm.transform(xtest['payment_sequential'].values.reshape(-1,1))\n",
        "    \n",
        "    norm=pickle.load(open('znorm6.pkl', 'rb'))\n",
        "    xte_phr=norm.transform(xtest['f13_order_purchase_hour'].values.reshape(-1,1))\n",
        "    \n",
        "\n",
        "    model=pickle.load(open('zstack_model.pkl', 'rb'))\n",
        "    \n",
        "    #here we are vectorising the text data using tfidf such that we are ignoring the terms which are appearing less than 3 times\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    vectorizer =pickle.load(open('zvectorizer.pkl', 'rb')) \n",
        "    xte_rcm=vectorizer.transform(xtest[\"review_comment_message\"].values)\n",
        "    \n",
        "    #we are stacking the horizontally the all the vectors formed\n",
        "    from scipy.sparse import hstack\n",
        "    xte=hstack((xte_pcm,xte_pty,xte_os,xte_pnl,xte_pdl,xte_apv,xte_psq,xte_pv,xte_phr,xte_pri,xte_pw,xte_pi,xte_dad,xte_wpr,xte_opd,xtedon,xte_po,xte_rcm)).tocsr()\n",
        "    y_true=ytest\n",
        "    print(len(ytest),xte.shape)\n",
        "    if count==9:\n",
        "      y_pred=model.predict(xte)\n",
        "      \n",
        "    from sklearn.metrics import f1_score\n",
        "    score=f1_score(y_true, y_pred, average='micro')\n",
        "    return score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LrV34OA5XhD8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=final_2(\"combine.xlsx\")\n",
        "print()\n",
        "print(\"\\nf1 score is\",prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-QZeS2HFekf",
        "outputId": "f0f5a8ec-0f74-4faf-8dcf-1db60c9d4dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data retained after doing doing data cleaning 99.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "100%|██████████| 98896/98896 [00:44<00:00, 2199.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30638 (30638, 2151)\n",
            "\n",
            "\n",
            "f1 score is 0.9201971408055356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZoPQxsnU6Cmd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}