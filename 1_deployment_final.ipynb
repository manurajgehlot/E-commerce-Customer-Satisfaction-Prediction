{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deployment_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmwjyBsf5HTv",
        "outputId": "4d883164-2b07-47d8-899d-2b06dacdec54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.12.2-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 42.7 MB/s \n",
            "\u001b[?25hCollecting watchdog\n",
            "  Downloading watchdog-2.1.9-py3-none-manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.21.6)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0b1-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Collecting rich>=10.11.0\n",
            "  Downloading rich-12.5.1-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 67.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (6.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.12.0)\n",
            "Collecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.4)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 65.2 MB/s \n",
            "\u001b[?25hCollecting blinker>=1.0.0\n",
            "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: requests>=2.4 in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 74.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.13.0)\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.3)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.3.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->streamlit) (3.8.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (5.9.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.1->streamlit) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.12->streamlit) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.4->streamlit) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.4->streamlit) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.4->streamlit) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.4->streamlit) (1.24.3)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 9.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.11.0->streamlit) (2.6.1)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators>=0.2->streamlit) (4.4.2)\n",
            "Building wheels for collected packages: validators\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=2c5149fef1635f78ed2ff6704d5a7f253a15be67c1c91073200178c8c619117a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\n",
            "Successfully built validators\n",
            "Installing collected packages: smmap, gitdb, commonmark, watchdog, validators, rich, pympler, pydeck, gitpython, blinker, streamlit\n",
            "Successfully installed blinker-1.5 commonmark-0.9.1 gitdb-4.0.9 gitpython-3.1.27 pydeck-0.8.0b1 pympler-1.0.1 rich-12.5.1 smmap-5.0.0 streamlit-1.12.2 validators-0.20.0 watchdog-2.1.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[K     |████████████████████████████████| 745 kB 34.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=b6b7dee32f3bc5fdab590d1943a882563bed3646e016fff7be5b6b24d471043e\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit \n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "S0MqS0hYM1Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "model=pickle.load(open('zstack_model.pkl', 'rb'))\n",
        "st.title('REVIEW SENTIMENT PREDICTION!!!')\n",
        "uploaded_file = st.file_uploader(\"Choose a file\")\n",
        "if st.button(\"predict\"):\n",
        "  if uploaded_file is not None: \n",
        "    count=0\n",
        "    xls = pd.ExcelFile(uploaded_file)\n",
        "    geo= pd.read_excel(xls,'geo',index_col=\"Unnamed: 0\")\n",
        "    item=pd.read_excel(xls,'item',index_col=\"Unnamed: 0\")\n",
        "    customer= pd.read_excel(xls,'customer',index_col=\"Unnamed: 0\")\n",
        "    review=pd.read_excel(xls,'review',index_col=\"Unnamed: 0\")\n",
        "    orders= pd.read_excel(xls,'orders',index_col=\"Unnamed: 0\")\n",
        "    products=pd.read_excel(xls,'products',index_col=\"Unnamed: 0\")\n",
        "    sellers= pd.read_excel(xls,'sellers',index_col=\"Unnamed: 0\")\n",
        "    payment=pd.read_excel(xls,'payment',index_col=\"Unnamed: 0\")\n",
        "\n",
        "    df=pd.merge(orders,payment,on=\"order_id\",how=\"inner\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df=pd.merge(df,customer,on=\"customer_id\",how=\"outer\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df=pd.merge(df,item,on=\"order_id\",how=\"inner\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df.drop_duplicates(subset=[\"order_id\"],inplace=True)\n",
        "    df=pd.merge(df,products,on=\"product_id\",how=\"inner\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df=pd.merge(df,review,on=\"order_id\",how=\"outer\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df=pd.merge(df,sellers,on=\"seller_id\",how=\"outer\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df.replace(to_replace=\"bebes\",\n",
        "              value=\"babies\")\n",
        "    df.replace(to_replace=\"ferramentas_jardim\",\n",
        "              value=\"garden_tools\")\n",
        "    df.replace(to_replace=\"informatica_acessorios\",\n",
        "              value=\"informatica_accessories\")\n",
        "    df[\"review_comment_message\"].fillna(\"nao_reveja\",inplace=True)\n",
        "    df.drop(columns=[\"review_answer_timestamp\",\"review_creation_date\",\"review_id\",\"review_comment_title\",\"order_delivered_carrier_date\"],axis=1,inplace=True)\n",
        "    df1=df.copy()\n",
        "    df1[\"review_score\"]=df1[\"review_score\"].apply(lambda x: 0 if x < 3 else 1)\n",
        "    df1[\"order_purchase_timestamp\"]=pd.to_datetime(df1[\"order_purchase_timestamp\"])\n",
        "    df1[\"order_approved_timestamp\"]=pd.to_datetime(df1[\"order_approved_at\"])\n",
        "    df1[\"order_purchase_date\"]=df1.order_purchase_timestamp.dt.date\n",
        "    df1[\"approval_date\"]=df1[\"order_approved_timestamp\"].dt.date\n",
        "    df1[\"order_estimated_delivery_date\"]=pd.to_datetime(df1.order_estimated_delivery_date).dt.date\n",
        "    df1[\"order_delivered_customer_date\"]=pd.to_datetime(df1.order_delivered_customer_date).dt.date\n",
        "    df1.drop_duplicates(subset=[\"order_id\",\"customer_id\",\"order_status\",\"order_status\",\"order_approved_at\",\"order_delivered_customer_date\",\"product_weight_g\",\"product_category_name_english\",\"review_comment_message\"],inplace=True)\n",
        "    print(\"data retained after doing doing data cleaning {:.1f}\".format((df1.shape[0]/df.shape[0])*100))\n",
        "    import nltk\n",
        "    nltk.download(\"stopwords\")\n",
        "    from nltk.corpus import stopwords\n",
        "    from tqdm import tqdm\n",
        "    from nltk.stem import RSLPStemmer\n",
        "    import re\n",
        "    nltk.download(\"rslp\")\n",
        "    stopwords_portugues=stopwords.words(\"portuguese\")\n",
        "    stopwords_portugues.remove(\"nem\")\n",
        "    stopwords_portugues.remove(\"não\")\n",
        "    stemmer=RSLPStemmer()\n",
        "    preprocessed_reviews = []\n",
        "    for sentance in tqdm(df1['review_comment_message'].values):\n",
        "        sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
        "        sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
        "        sentance=re.sub(\"[nN][ãaAÃ][oO][Ññ]|[nN]\",\"Negação\",sentance)\n",
        "        sentance=sentance.replace(\"\\\\r\",'')\n",
        "        sentance=sentance.replace(\"\\\\n\",'')\n",
        "        sentance = re.sub('[^A-Za-z]+',' ',sentance)\n",
        "        sentance = ' '.join(a for a in sentance.split() if a.lower not in stopwords_portugues)\n",
        "        sentance = ' '.join(stemmer.stem(e.lower())for e in sentance.split())\n",
        "        \n",
        "        preprocessed_reviews.append(sentance.strip())\n",
        "\n",
        "\n",
        "    f10=df1.groupby(\"order_id\").count()[\"product_id\"]\n",
        "    f10.df=pd.DataFrame()\n",
        "    f10.df[\"order_id\"]=f10.index\n",
        "    f10.df[\"F10_number_of_product_ordered\"]=f10.values\n",
        "    f9=df1.groupby(\"product_id\").count()[\"seller_id\"]\n",
        "    f9.df=pd.DataFrame()\n",
        "    f9.df[\"product_id\"]=f9.index\n",
        "    f9.df[\"number_of_sellers\"]=f9.values\n",
        "    df1[\"f16_purchase_month\"]=(df1[\"order_purchase_timestamp\"]).apply(lambda x: x.month)\n",
        "    df1[\"f14_delivery_days\"]=(df1[\"order_delivered_customer_date\"]-df1[\"order_purchase_date\"]).dt.days\n",
        "    df1=pd.merge(df1,f9.df,on=\"product_id\")\n",
        "    df1=pd.merge(df1,f10.df,on=\"order_id\")\n",
        "\n",
        "    df1[\"f1_delivery_estimated_days\"]=(df1[\"approval_date\"]-df1[\"order_estimated_delivery_date\"]).dt.days\n",
        "    df1[\"f2_delivery_actual_days\"]=(df1[\"approval_date\"]-df1[\"order_delivered_customer_date\"]).dt.days\n",
        "    df1[\"f3_on_time_or_late\"]=(df1[\"f1_delivery_estimated_days\"] > df1[\"f2_delivery_actual_days\"]).astype(\"int\")\n",
        "    df1[\"f4_discount_or_not\"]=(df1[\"payment_value\"]-(df1[\"price\"]+df1[\"freight_value\"])).apply(lambda x: 1 if x < 0 else 0)#converted to boolean \n",
        "    df1[\"f5_purchase_approved_delay\"]=(df1[\"approval_date\"]-df1[\"order_purchase_date\"]).dt.days\n",
        "    df1[\"f6_Average_product_value_per_order\"]=df1.price/(df1.F10_number_of_product_ordered)\n",
        "    df1[\"f7_order_frieght_value\"]=df1.freight_value/df1.price\n",
        "    df1[\"f8_purchase_day_week\"]=df1.order_purchase_timestamp.dt.dayofweek\n",
        "    df1[\"f11_order_purch_day\"]=df1.order_purchase_timestamp.dt.day\n",
        "    df1[\"f12_words_per_review\"]=df1.review_comment_message.apply(lambda x: len(x.split(\" \")))\n",
        "    df1[\"f13_order_purchase_hour\"]=df1[\"order_purchase_timestamp\"].apply(lambda x: x.hour)\n",
        "    df1[\"f15_week_delivery\"]=df1[\"order_purchase_timestamp\"].apply(lambda x: x.dayofweek)\n",
        "\n",
        "\n",
        "    df1[\"review_comment_message\"]=preprocessed_reviews\n",
        "\n",
        "\n",
        "    \n",
        "    xtest=df1.copy()\n",
        "    xtest.drop(['order_id'],axis=1,inplace=True)\n",
        "    xtest=xtest[[\"product_description_lenght\",\"product_name_lenght\",\"order_status\",\"payment_type\",\"product_category_name_english\",\"payment_installments\",\"product_weight_g\",\"price\",\"payment_sequential\",\"product_photos_qty\",\"f13_order_purchase_hour\",\"f6_Average_product_value_per_order\",\"payment_value\",\"f11_order_purch_day\",\"f12_words_per_review\",'f2_delivery_actual_days','f3_on_time_or_late', 'f4_discount_or_not','F10_number_of_product_ordered','review_score','review_comment_message']]\n",
        "    xtedon=xtest[['f3_on_time_or_late', 'f4_discount_or_not']]\n",
        "    \n",
        "    if 0<xtest['f6_Average_product_value_per_order'].values<7000:\n",
        "        norm=pickle.load(open('znorm1.pkl', 'rb'))\n",
        "        xte_apv=norm.transform(xtest['f6_Average_product_value_per_order'].values.reshape(-1,1))\n",
        "        count+=1\n",
        "    else:\n",
        "        st.write(\"enter correct value of average product value per order between 0-7000\")\n",
        "        xte_apv=0\n",
        "    \n",
        "    \n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    \n",
        "    if 3 < xtest[\"product_description_lenght\"].values < 4000:\n",
        "        norm=pickle.load(open('znorm3.pkl', 'rb'))\n",
        "        xte_pdl=norm.transform(xtest[\"product_description_lenght\"].values.reshape(-1,1))\n",
        "        count+=1\n",
        "    else:\n",
        "         st.write(\"enter correct product description length between 3-4000\")\n",
        "         xte_pdl=0\n",
        "        \n",
        "    if 4 < xtest[\"product_name_lenght\"].values < 80:\n",
        "        norm=pickle.load(open('znorm4.pkl', 'rb'))\n",
        "        xte_pnl=norm.transform(xtest[\"product_name_lenght\"].values.reshape(-1,1))\n",
        "        count+=1\n",
        "    \n",
        "    else:\n",
        "      st.write(\"enter correct product name length between 4-80\")\n",
        "      xte_pnl=0   \n",
        "   \n",
        "    if 0 < xtest[\"payment_value\"].values < 14000:\n",
        "        norm=pickle.load(open('znorm5.pkl', 'rb'))\n",
        "        xte_pv=norm.transform(xtest[\"payment_value\"].values.reshape(-1,1))\n",
        "        count+=1\n",
        "    else:\n",
        "        st.write(\"enter valid payment value between 0-14000\")\n",
        "        xte_pv=0\n",
        "    \n",
        "    if 0 < xtest[\"price\"].values < 7000:\n",
        "          norm=pickle.load(open('znorm7.pkl', 'rb'))\n",
        "          xte_pri=norm.transform(xtest['price'].values.reshape(-1,1))\n",
        "          count+=1\n",
        "    else:\n",
        "       st.write(\"enter the correct price between 0-7000\")\n",
        "       xte_pri=0\n",
        "    \n",
        "\n",
        "    if 0 < xtest['product_weight_g'].values < 41000:\n",
        "\n",
        "      norm=pickle.load(open('znorm8.pkl', 'rb'))\n",
        "      xte_pw=norm.transform(xtest['product_weight_g'].values.reshape(-1,1))\n",
        "      count+=1\n",
        "    \n",
        "    else:\n",
        "       st.write(\"enter correct product weight between 0-41000\")\n",
        "       xte_pw=0  \n",
        "    \n",
        "    if 0<=xtest['payment_installments'].values<=30:\n",
        "      norm=pickle.load(open('znorm9.pkl', 'rb'))\n",
        "      xte_pi=norm.transform(xtest['payment_installments'].values.reshape(-1,1))\n",
        "      count+=1\n",
        "    else:\n",
        "      print(\"correct payment insatllments 0-30\")\n",
        "      xte_pi=0\n",
        "\n",
        "    #here we are normalising the numerical features as so that all come to same scale\n",
        "    norm=pickle.load(open('znorm10.pkl', 'rb'))\n",
        "    xte_dad=norm.transform(xtest[\"f2_delivery_actual_days\"].values.reshape(-1,1))\n",
        "     \n",
        "\n",
        "    norm=pickle.load(open('norm11.pkl', 'rb'))\n",
        "    xte_opd=norm.transform(xtest[\"f11_order_purch_day\"].values.reshape(-1,1))\n",
        "    \n",
        "    if 0< xtest[\"f12_words_per_review\"].values <60:\n",
        "      norm=pickle.load(open('znorm12.pkl', 'rb'))\n",
        "      xte_wpr=norm.transform(xtest[\"f12_words_per_review\"].values.reshape(-1,1))\n",
        "      count+=1\n",
        "    else:\n",
        "      st.write(\"enter limited words in review comment message between  0-60 \")\n",
        "      xte_wpr=0\n",
        "    \n",
        "    \n",
        "    if 0< xtest['F10_number_of_product_ordered'].values < 10:\n",
        "\n",
        "      norm=pickle.load(open('znorm13.pkl', 'rb'))\n",
        "      xte_po=norm.transform(xtest['F10_number_of_product_ordered'].values.reshape(-1,1))\n",
        "      count+=1\n",
        "    else:\n",
        "      st.write(\"enter correct number of products ordered between 0-10 \")\n",
        "      xte_po=0\n",
        "\n",
        "\n",
        "    vect=pickle.load(open('zvect.pkl', 'rb'))\n",
        "    xte_os=vect.transform(xtest[\"order_status\"].values)\n",
        "\n",
        "\n",
        "\n",
        "    vect=pickle.load(open('zvect1.pkl', 'rb'))\n",
        "    xte_pty=vect.transform(xtest[\"payment_type\"].values)\n",
        "\n",
        "\n",
        "    vect=pickle.load(open('zvect2.pkl', 'rb'))\n",
        "    xte_pcm=vect.transform(xtest[\"product_category_name_english\"].values)\n",
        "\n",
        "    norm=pickle.load(open('znorm2.pkl', 'rb'))\n",
        "    xte_psq=norm.transform(xtest['payment_sequential'].values.reshape(-1,1))\n",
        "    \n",
        "    norm=pickle.load(open('znorm6.pkl', 'rb'))\n",
        "    xte_phr=norm.transform(xtest['f13_order_purchase_hour'].values.reshape(-1,1))\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "    #here we are vectorising the text data using tfidf such that we are ignoring the terms which are appearing less than 3 times\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    vectorizer =pickle.load(open('zvectorizer.pkl', 'rb')) \n",
        "    xte_rcm=vectorizer.transform(xtest[\"review_comment_message\"].values)\n",
        "    \n",
        "    #we are stacking the horizontally the all the vectors formed\n",
        "    from scipy.sparse import hstack\n",
        "    xte=hstack((xte_pcm,xte_pty,xte_os,xte_pnl,xte_pdl,xte_apv,xte_psq,xte_pv,xte_phr,xte_pri,xte_pw,xte_pi,xte_dad,xte_wpr,xte_opd,xtedon,xte_po,xte_rcm)).tocsr()\n",
        "    \n",
        "    if count==9:\n",
        "      val=model.predict(xte)\n",
        "      if val==1:\n",
        "        st.write(\"positive review given\")\n",
        "      if val==0:\n",
        "        st.write(\"negative review given\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "moU88lRgfut5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584ae9d5-b2ac-4cd0-c2cb-2595a9c6e49e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token('2DQlpVpnhSIFXKSBhEU0y6h4Qw6_4UAwqZpWmhh5EbLzqudSz')"
      ],
      "metadata": {
        "id": "Qp27DUPb5qjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f500d2-6d09-4dd4-f9dd-4971e67aec75"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "n_G0tv7i8PgX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run app.py --server.port 80 &\n",
        "url = ngrok.connect(port = '80')\n",
        "print(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJqcIKyV6hVA",
        "outputId": "02c04335-4f75-4e03-b2f9-529a32c505f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "NgrokTunnel: \"http://b4c9-34-86-179-206.ngrok.io\" -> \"http://localhost:80\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B3N1YglkuL1E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}